{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5bebcc",
   "metadata": {},
   "source": [
    "# 4 Decision Trees, Ensembling and Double Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a202f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"hw2_fashionmnist/train.npy\")\n",
    "y_train = np.load(\"hw2_fashionmnist/trainlabels.npy\")\n",
    "x_test = np.load(\"hw2_fashionmnist/test.npy\")\n",
    "y_test = np.load(\"hw2_fashionmnist/testlabels.npy\")\n",
    "class_names = ['T-shirt/top','trouser','pullover','dress','coat','sandal','shirt','sneaker','bag','ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "for i in range(10):\n",
    "  plt.subplot(1,10,i+1) #creates 25 subplots in the image\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(x_train[i])\n",
    "  plt.xlabel(class_names[y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ba43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a720f",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471c124",
   "metadata": {},
   "source": [
    "(ii) As a first step, train a series of decision trees on the training split of FashionMNIST, with a varying limit on the maximum number of permitted leaf nodes. Once trained, evaluate the performance of your classifiers on both the train and test splits, plotting the 0-1 loss of the train/test curves against the maximum permitted number of leaf nodes (log scale horizontal axis). You are permitted to use an open source implementation of a decision tree classifier (such as sklearn’s DecisionTreeClassifier) as long as you are able to control the maximum number of leaves. What is the minimum loss you can achieve and what do you observe on the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c29fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(xtrain, y_train)\n",
    "tree.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain, h, w = x_train.shape\n",
    "ntest, h, w = x_test.shape\n",
    "xtrain = x_train.reshape((ntrain, h*w))\n",
    "xtest = x_test.reshape((ntest, h*w))\n",
    "max_leaves = [2, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000, 10000, 20000]\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "actual_leaves = []\n",
    "for max_leaf in max_leaves:\n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes = max_leaf)\n",
    "    tree.fit(xtrain, y_train)\n",
    "    train_loss.append(1 - tree.score(xtrain, y_train))\n",
    "    test_loss.append(1 - tree.score(xtest, y_test))\n",
    "    actual_leaves.append(tree.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = min(test_loss)\n",
    "print('minimum loss is',min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('max number of leaf nodes')\n",
    "plt.ylabel('0-1 loss')\n",
    "plt.plot(max_leaves, train_loss, label = 'train')\n",
    "plt.plot(max_leaves, test_loss, label = 'test')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "plt.ylabel('number of leaves')\n",
    "plt.plot(max_leaves, train_loss, label = 'train')\n",
    "plt.plot(max_leaves, test_loss, label = 'test')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90f5c9",
   "metadata": {},
   "source": [
    "(v) With the random forest model, we now have two hyperparameters to control: the number of estimators and the maximum permitted leaves in each estimator, making the total parameter count the product of the two. In the ensuing sections, you are allowed to use an open source implementation of the random forest classifier (such as sklearn’s RandomForestClassifier) as long as you can control the number of estimators used and maximum number of leaves in each decision tree trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c4d2f",
   "metadata": {},
   "source": [
    "(a) First, make a plot measuring the train and test 0-1 loss of a random forest classifier with a fixed number of estimators (default works just fine) but with varying number of maximum allowed tree leaves for individual estimators. You should plot the train and test error on the same axis against a log scale of the total number of parameters on the horizontal axis. In this case, you are making individual classifiers more powerful but keeping the size of the forest the same. What do you observe- does an overfit seem possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02469406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of estimators = 100 (default)\n",
    "max_leaf1 = [2, 100, 500, 1000, 2000, 3000, 4000, 5000, 7000]\n",
    "forest_train_loss = []\n",
    "forest_test_loss = []\n",
    "for max_leaf in max_leaf1:\n",
    "    forest = RandomForestClassifier(max_leaf_nodes = max_leaf)\n",
    "    forest.fit(xtrain, y_train)\n",
    "    forest_train_loss.append(1 - forest.score(xtrain, y_train))\n",
    "    forest_test_loss.append(1 - forest.score(xtest, y_test))\n",
    "\n",
    "num_param = [i * 100 for i in max_leaves]\n",
    "plt.xlabel('total number of parameters, fixed #estimators=100')\n",
    "plt.ylabel('0-1 loss')\n",
    "plt.plot(num_param, forest_train_loss, label = 'train')\n",
    "plt.plot(num_param, forest_test_loss, label = 'test')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af18bfd",
   "metadata": {},
   "source": [
    "(b) Second, make a plot measuring the train and test 0-1 loss of a random forest classifier with a fixed maximum number of leaves but varying number of estimators. You should plot the train and test error on the same axis against a log scale of the total number of parameters on the horizontal axis. Ensure that the maximum number of leaves permitted is small compared to your answer in part (iii) to have shallower trees. In this case, you are making the whole forest larger without allowing any individual tree to fit the data perfectly, aka without any individual tree achieving zero empirical risk. How does your best loss compare to the best loss achieved with a single decision tree? What about for a similar number of total parameters? With a sufficiently large number of estimators chosen, you should still see variance increasing, albeit with an overall lower test loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ecc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of estimators = 100 (default)\n",
    "num_estimator = [10, 50, 100, 200, 300, 400, 500]\n",
    "f_train_loss = []\n",
    "f_test_loss = []\n",
    "for max_leaf in max_leaves:\n",
    "    f = RandomForestClassifier(max_leaf_nodes = 100, n_estimators = num_estimator)\n",
    "    f.fit(xtrain, y_train)\n",
    "    f_train_loss.append(1 - forest.score(xtrain, y_train))\n",
    "    f_test_loss.append(1 - forest.score(xtest, y_test))\n",
    "\n",
    "num_param = [i * 100 for i in max_leaves]\n",
    "plt.xlabel('total number of parameters, fixed max_leaf_nodes=100')\n",
    "plt.ylabel('0-1 loss')\n",
    "plt.plot(num_param, f_train_loss, label = 'train')\n",
    "plt.plot(num_param, f_test_loss, label = 'test')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663aa10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

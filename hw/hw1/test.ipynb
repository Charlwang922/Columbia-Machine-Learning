{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the human train set: 3233418\n",
      "Total number of words in the GPT train set: 3115935\n",
      "Total number of words in the human test set: 359269\n",
      "Total number of words in the GPT test set: 346216\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define the filenames\n",
    "hum_file = \"hum.txt\"\n",
    "gpt_file = \"gpt.txt\"\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove all punctuation except ,.?!\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation.replace(\",.?!\",\"\"))\n",
    "    text = text.translate(translator)\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Add special <START> and <END> tokens\n",
    "    text = \"<START> \" + text + \" <END>\"\n",
    "    return text\n",
    "\n",
    "# Read in the human and GPT text files\n",
    "with open(hum_file, encoding='utf-8') as f:\n",
    "    hum_text = f.read()\n",
    "with open(gpt_file, encoding='utf-8') as f:\n",
    "    gpt_text = f.read()\n",
    "\n",
    "# Clean the text\n",
    "hum_text = clean_text(hum_text)\n",
    "gpt_text = clean_text(gpt_text)\n",
    "\n",
    "# Split the text into lists of words\n",
    "hum_words = hum_text.split()\n",
    "gpt_words = gpt_text.split()\n",
    "\n",
    "# Partition 90% of the data to a training set and 10% to test set\n",
    "hum_train_len = int(len(hum_words) * 0.9)\n",
    "gpt_train_len = int(len(gpt_words) * 0.9)\n",
    "\n",
    "hum_train = hum_words[:hum_train_len]\n",
    "hum_test = hum_words[hum_train_len:]\n",
    "\n",
    "gpt_train = gpt_words[:gpt_train_len]\n",
    "gpt_test = gpt_words[gpt_train_len:]\n",
    "\n",
    "\n",
    "print(\"Total number of words in the human train set:\", len(hum_train))\n",
    "print(\"Total number of words in the GPT train set:\", len(gpt_train))\n",
    "print(\"Total number of words in the human test set:\", len(hum_test))\n",
    "print(\"Total number of words in the GPT test set:\", len(gpt_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_bigram_freqs, hum_trigram_freqs, hum_oov_rate_bigrams, hum_oov_rate_trigrams = train_model(hum_train, hum_test)\n",
    "gpt_bigram_freqs, gpt_trigram_freqs, gpt_oov_rate_bigrams, gpt_oov_rate_trigrams = train_model(gpt_train, gpt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18341131575504704,\n",
       " 0.5303352084371321,\n",
       " 0.12031217505834509,\n",
       " 0.3854559003627793)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hum_oov_rate_bigrams, hum_oov_rate_trigrams, gpt_oov_rate_bigrams, gpt_oov_rate_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(877399, 2113914)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hum_bigram_freqs), len(hum_trigram_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "hum_unigram_freqs = Counter(hum_train)\n",
    "gpt_unigram_freqs = Counter(gpt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57386, 37794)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hum_unigram_freqs), len(gpt_unigram_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Build frequency tables for bigram and trigram models\n",
    "hum_bigram_freqs = Counter(zip(hum_train[:-1], hum_train[1:]))\n",
    "hum_trigram_freqs = Counter(zip(hum_train[:-2], hum_train[1:-1], hum_train[2:]))\n",
    "gpt_bigram_freqs = Counter(zip(gpt_train[:-1], gpt_train[1:]))\n",
    "gpt_trigram_freqs = Counter(zip(gpt_train[:-2], gpt_train[1:-1], gpt_train[2:]))\n",
    "\n",
    "# Calculate conditional probabilities for bigram and trigram models\n",
    "hum_bigram_probs = {}\n",
    "hum_trigram_probs = {}\n",
    "for bigram, count in hum_bigram_freqs.items():\n",
    "    prev_word = bigram[0]\n",
    "    hum_bigram_probs[bigram] = count / hum_unigram_freqs[prev_word]\n",
    "for trigram, count in hum_trigram_freqs.items():\n",
    "    prev_two_words = trigram[:2]\n",
    "    hum_trigram_probs[trigram] = count / hum_bigram_freqs[prev_two_words]\n",
    "\n",
    "gpt_bigram_probs = {}\n",
    "gpt_trigram_probs = {}\n",
    "for bigram, count in gpt_bigram_freqs.items():\n",
    "    prev_word = bigram[0]\n",
    "    gpt_bigram_probs[bigram] = count / gpt_unigram_freqs[prev_word]\n",
    "for trigram, count in gpt_trigram_freqs.items():\n",
    "    prev_two_words = trigram[:2]\n",
    "    gpt_trigram_probs[trigram] = count / gpt_bigram_freqs[prev_two_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7067059021975016"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify sentences in test sets as human or AI generated\n",
    "test_probs = []\n",
    "for i in range(len(hum_test)-1):\n",
    "    bigram = tuple(hum_test[i:i+2])\n",
    "    if bigram in hum_bigram_probs:\n",
    "        p1 = hum_bigram_probs[bigram]\n",
    "    else:\n",
    "        p1 = 0\n",
    "    if bigram in gpt_bigram_probs:\n",
    "        p2 = gpt_bigram_probs[bigram]\n",
    "    else:\n",
    "        p2 = 0\n",
    "    \n",
    "    test_probs.append(p1 >= p2)\n",
    "\n",
    "for i in range(len(gpt_test)-1):\n",
    "    bigram = tuple(gpt_test[i:i+2])\n",
    "    if bigram in hum_bigram_probs:\n",
    "        p1 = hum_bigram_probs[bigram]\n",
    "    else:\n",
    "        p1 = 0\n",
    "    if bigram in gpt_bigram_probs:\n",
    "        p2 = gpt_bigram_probs[bigram]\n",
    "    else:\n",
    "        p2 = 0\n",
    "    \n",
    "    test_probs.append(p1 <= p2)\n",
    "\n",
    "sum(test_probs) / len(test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8456429017932446"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify sentences in test sets as human or AI generated\n",
    "test_probs = []\n",
    "for i in range(len(hum_test)-2):\n",
    "    bigram = tuple(hum_test[i:i+3])\n",
    "    if bigram in hum_trigram_probs:\n",
    "        p1 = hum_trigram_probs[bigram]\n",
    "    else:\n",
    "        p1 = 0\n",
    "    if bigram in gpt_trigram_probs:\n",
    "        p2 = gpt_trigram_probs[bigram]\n",
    "    else:\n",
    "        p2 = 0\n",
    "    \n",
    "    test_probs.append(p1 >= p2)\n",
    "\n",
    "for i in range(len(gpt_test)-2):\n",
    "    bigram = tuple(gpt_test[i:i+3])\n",
    "    if bigram in hum_trigram_probs:\n",
    "        p1 = hum_trigram_probs[bigram]\n",
    "    else:\n",
    "        p1 = 0\n",
    "    if bigram in gpt_trigram_probs:\n",
    "        p2 = gpt_trigram_probs[bigram]\n",
    "    else:\n",
    "        p2 = 0\n",
    "    \n",
    "    test_probs.append(p1 <= p2)\n",
    "\n",
    "sum(test_probs) / len(test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def ngram_freq(tokens, n, T):\n",
    "    def make_all_w_condition(all_unique_w, n_1_previous_tokens):\n",
    "        return [n_1_previous_tokens + (w,) for w in all_unique_w]\n",
    "    all_unique_w = set(tokens)\n",
    "    freq = {}\n",
    "    for i in range(len(tokens)+1-n):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        if ngram in freq:\n",
    "            freq[ngram]+=1\n",
    "        else:\n",
    "            freq[ngram]=1\n",
    "    freq_array = np.array(list(frequncies.values()))\n",
    "    exp_array = np.exp(freq_array/T)\n",
    "    frequncies=dict(zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dce9ce0b8994960befdaf1abc06919148bccb19973be1d090d69590bd56698c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
